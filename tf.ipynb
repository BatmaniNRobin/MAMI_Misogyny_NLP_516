{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duyiYKj_clff"
      },
      "source": [
        "Importing Libraries and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOXWY9iavpyb",
        "outputId": "68d04c0a-d83e-4b08-f913-0557ddcee6bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Requirement already satisfied: utils in /Users/mani/miniforge3/envs/cloud/lib/python3.9/site-packages (1.0.1)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Requirement already satisfied: setuptools in /Users/mani/miniforge3/envs/cloud/lib/python3.9/site-packages (59.5.0)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Requirement already satisfied: numpy in /Users/mani/miniforge3/envs/cloud/lib/python3.9/site-packages (1.21.4)\n"
          ]
        }
      ],
      "source": [
        "# Mohammad Malik & Adair Maynard\n",
        "# Task 5: MAMI\n",
        "# CMSC 516: NLP at VCU, Dr. Bridget McInnes\n",
        "\n",
        "!pip install utils\n",
        "!pip install setuptools\n",
        "!pip install numpy\n",
        "\n",
        "import os\n",
        "import io\n",
        "import utils\n",
        "import re\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from math import floor\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "import tensorflow as tf\n",
        "import transformers as trfs\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "# from google.colab import auth\n",
        "# from googleapiclient.discovery import build\n",
        "# from io import FileIO\n",
        "# from googleapiclient.http import MediaIoBaseDownload\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation \n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "aPec8z7DwFCx",
        "outputId": "ea52c0b6-7d84-4047-8173-a53844769eb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.12.5\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'2.6.0'"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(trfs.__version__)\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mE0G90Czwq4z",
        "outputId": "e71d1b4d-8a18-431a-d5f4-a1d0875a6ce7"
      },
      "outputs": [],
      "source": [
        "#  Get the files from the google drive\n",
        "# auth.authenticate_user()\n",
        "# drive_service = build('drive', 'v3')\n",
        "\n",
        "# # Get data file\n",
        "# file_id = '1sgE4tBtZwE-M9Y6rfLu_FlwI6VpnaUo-'  # Training file on the Google Drive\n",
        "# downloaded = io.FileIO(\"training.csv\", 'w')\n",
        "# request = drive_service.files().get_media(fileId=file_id)\n",
        "# downloader = MediaIoBaseDownload(downloaded, request)\n",
        "# done = False\n",
        "# while done is False:\n",
        "#   status, done = downloader.next_chunk()\n",
        "#   print(\"Download {}%.\".format(int(status.progress() * 100)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwG-Ix8Mwr2k",
        "outputId": "d1cdde71-4bac-4e6b-b66c-30ad5dabb221"
      },
      "outputs": [],
      "source": [
        "#  Load Tokenizer\n",
        "# TODO look at the finetuned model because it resulted in a lot of UNKNOWN words for capitals and doesnt know a lot of words\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\") #Hate-speech-CNERG/dehatebert-mono-english\n",
        "\n",
        "# Load Model\n",
        "# model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\") #Hate-speech-CNERG/dehatebert-mono-english"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "GmBalKfrwr0E",
        "outputId": "eb7d23ce-c80c-478f-a052-1f97bc4cdf6d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>misogynous</th>\n",
              "      <th>shaming</th>\n",
              "      <th>stereotype</th>\n",
              "      <th>objectification</th>\n",
              "      <th>violence</th>\n",
              "      <th>Text Transcription</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Milk Milk.zip</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>ROSES ARE RED, VIOLETS ARE BLUE IF YOU DON'T S...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1000.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>BREAKING NEWS: Russia releases photo of DONALD...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10000.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>MAN SEEKING WOMAN Ignad 18 O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10006.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Me explaining the deep lore of. J.R.R. Tolkein...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   file_name  misogynous  shaming  stereotype  objectification  violence  \\\n",
              "0      1.jpg           0        0           0                0         0   \n",
              "1     10.jpg           1        0           0                0         1   \n",
              "2   1000.jpg           0        0           0                0         0   \n",
              "3  10000.jpg           0        0           0                0         0   \n",
              "4  10006.jpg           0        0           0                0         0   \n",
              "\n",
              "                                  Text Transcription  \n",
              "0                                      Milk Milk.zip  \n",
              "1  ROSES ARE RED, VIOLETS ARE BLUE IF YOU DON'T S...  \n",
              "2  BREAKING NEWS: Russia releases photo of DONALD...  \n",
              "3                       MAN SEEKING WOMAN Ignad 18 O  \n",
              "4  Me explaining the deep lore of. J.R.R. Tolkein...  "
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load dataset\n",
        "df = pd.read_csv(\"TRAINING/training.csv\", delimiter=\"\\t\")\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "IMbXswpnwrxu",
        "outputId": "2e7eed66-14b2-4da1-aa2c-9c06d5ba7f69"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>misogynous</th>\n",
              "      <th>Text Transcription</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>Milk Milk.zip</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>ROSES ARE RED, VIOLETS ARE BLUE IF YOU DON'T S...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1000.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>BREAKING NEWS: Russia releases photo of DONALD...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10000.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>MAN SEEKING WOMAN Ignad 18 O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10006.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>Me explaining the deep lore of. J.R.R. Tolkein...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>15002.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>WAITING FOR THE END OF THE COVID  imgflip.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>15003.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>SMART WOMEN ARE AROUND  imgflip.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>15004.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>GOOD GIRLS ARE BEHIND THE CORNER  imgflip.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>15005.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>COOKING FOR MY WIFE  imgflip.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>15006.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>LISTEN TOMORROW WILL BE MONDAY imgflip.com FRO...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      file_name  misogynous                                 Text Transcription\n",
              "0         1.jpg           0                                      Milk Milk.zip\n",
              "1        10.jpg           1  ROSES ARE RED, VIOLETS ARE BLUE IF YOU DON'T S...\n",
              "2      1000.jpg           0  BREAKING NEWS: Russia releases photo of DONALD...\n",
              "3     10000.jpg           0                       MAN SEEKING WOMAN Ignad 18 O\n",
              "4     10006.jpg           0  Me explaining the deep lore of. J.R.R. Tolkein...\n",
              "...         ...         ...                                                ...\n",
              "9995  15002.jpg           0      WAITING FOR THE END OF THE COVID  imgflip.com\n",
              "9996  15003.jpg           0                SMART WOMEN ARE AROUND  imgflip.com\n",
              "9997  15004.jpg           0      GOOD GIRLS ARE BEHIND THE CORNER  imgflip.com\n",
              "9998  15005.jpg           0                   COOKING FOR MY WIFE  imgflip.com\n",
              "9999  15006.jpg           0  LISTEN TOMORROW WILL BE MONDAY imgflip.com FRO...\n",
              "\n",
              "[10000 rows x 3 columns]"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# get rid of multi classes and only keep misogyny\n",
        "X = df.drop(columns=['shaming', 'objectification', 'stereotype', 'violence'])\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3tyXdLBwrur",
        "outputId": "9d9b573c-e02a-47a6-8eeb-0a7eb3aa9f84"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /Users/mani/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /Users/mani/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/mani/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# preprocess text then pass into model\n",
        "\n",
        "# download NLTK stopwords, punctuation and wordLemmatizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# clean/preprocess dataset instances\n",
        "def clean_data(trans):\n",
        "  words = stopwords.words('english')\n",
        "  filtered_words = \" \".join([w for w in trans.split() if not w.lower() in words]) # might need to be trans.lower().split()\n",
        "  url = ''.join(re.sub(r'http\\S+', '', filtered_words))\n",
        "  almost = ''.join(re.sub(r'www\\S+', '', url))\n",
        "  cleaned = ''.join(re.sub(r'(\\w+\\.\\w+)', '', almost))\n",
        "  punct = ''.join(ch for ch in cleaned if ch not in punctuation)\n",
        "  wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "  word_tokens = nltk.word_tokenize(punct)\n",
        "  lemmatized_word = [wordnet_lemmatizer.lemmatize(word) for word in word_tokens]\n",
        "\n",
        "  word_joined = \" \".join(lemmatized_word)\n",
        "  \n",
        "  return word_joined"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "LVWi-IPfwrsK"
      },
      "outputs": [],
      "source": [
        "# apply preprocessing to dataset\n",
        "X['Cleaned Text'] = X['Text Transcription'].apply(clean_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "jPGSIYRkwrnH",
        "outputId": "bd0bf882-c74d-4128-97bc-2c53d9ffdeee"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Misogynous</th>\n",
              "      <th>Cleaned_Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Milk</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>ROSES RED VIOLETS BLUE SAY YES ILL RAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>BREAKING NEWS Russia release photo DONALD TRUM...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>MAN SEEKING WOMAN Ignad 18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>explaining deep lore of R Tolkeins world Arda ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>0</td>\n",
              "      <td>WAITING END COVID</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>0</td>\n",
              "      <td>SMART WOMEN AROUND</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>0</td>\n",
              "      <td>GOOD GIRLS BEHIND CORNER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>0</td>\n",
              "      <td>COOKING WIFE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>0</td>\n",
              "      <td>LISTEN TOMORROW MONDAY FROM USERMENSWEARHOUSE</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Misogynous                                       Cleaned_Text\n",
              "0              0                                               Milk\n",
              "1              1            ROSES RED VIOLETS BLUE SAY YES ILL RAPE\n",
              "2              0  BREAKING NEWS Russia release photo DONALD TRUM...\n",
              "3              0                         MAN SEEKING WOMAN Ignad 18\n",
              "4              0  explaining deep lore of R Tolkeins world Arda ...\n",
              "...          ...                                                ...\n",
              "9995           0                                  WAITING END COVID\n",
              "9996           0                                 SMART WOMEN AROUND\n",
              "9997           0                           GOOD GIRLS BEHIND CORNER\n",
              "9998           0                                       COOKING WIFE\n",
              "9999           0      LISTEN TOMORROW MONDAY FROM USERMENSWEARHOUSE\n",
              "\n",
              "[10000 rows x 2 columns]"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# create new variable that solely consists of cleaned text and labels\n",
        "X_bert = pd.DataFrame({\n",
        "    # 'File_Name':X['file_name'],\n",
        "    'Misogynous':X['misogynous'],\n",
        "    # 'Alpha':['a']*X.shape[0],\n",
        "    'Cleaned_Text':X['Cleaned Text']\n",
        "})\n",
        "X_bert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Tkmquqjy1xf",
        "outputId": "fa5fa233-8610-4477-b04b-06da1e14b615"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(10000,)\n",
            "[0 1 0 ... 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "# get list of sentences and labels\n",
        "sentences = X_bert.Cleaned_Text.values\n",
        "print(sentences.shape)\n",
        "\n",
        "labels = X_bert.Misogynous.values\n",
        "print(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "fFPFrM2gwrkp"
      },
      "outputs": [],
      "source": [
        "# split dataset into train and test to be passed into models, 85% and 15%\n",
        "X_train, X_rem, y_train, y_rem = train_test_split(sentences, labels, train_size=0.85)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "iK3OlSxKwriR"
      },
      "outputs": [],
      "source": [
        "# split dataset further into valid and test\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_rem, y_rem, train_size=0.333)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "iuZ10uE0wrfq"
      },
      "outputs": [],
      "source": [
        "# print(X_train.shape)\n",
        "# print(y_train.shape)\n",
        "\n",
        "# print(X_valid.shape)\n",
        "# print(y_valid.shape)\n",
        "\n",
        "# print(X_test.shape)\n",
        "# print(y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2Sc2B17wrdL",
        "outputId": "c6b26cf1-1641-4952-c486-22fec1b19e81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "orig:  REDDIT CAKE DAY ALMOST DONT POST ANYTHING excuse fuck almost forgot post\n",
            "Tokenized:  ['red', '##dit', 'cake', 'day', 'almost', 'don', '##t', 'post', 'anything', 'excuse', 'fuck', 'almost', 'forgot', 'post']\n",
            "Token IDs:  [2417, 23194, 9850, 2154, 2471, 2123, 2102, 2695, 2505, 8016, 6616, 2471, 9471, 2695]\n"
          ]
        }
      ],
      "source": [
        "# testing bert tokenizer\n",
        "print('orig: ', X_train[1])\n",
        "print('Tokenized: ', tokenizer.tokenize(X_train[1]))\n",
        "print(\"Token IDs: \", tokenizer.convert_tokens_to_ids(tokenizer.tokenize(X_train[1])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgAPwhIe4iUv",
        "outputId": "e3675cfb-c813-442b-9d5e-b35a578b4ff7"
      },
      "outputs": [],
      "source": [
        "# for idx in X_train:\n",
        "#   # Tokenize the text and add `[CLS]` and `[SEP]` tokens\n",
        "#   input_ids = tokenizer.encode(idx, add_special_tokens=True)\n",
        "#   # Update the maximum sentence length\n",
        "#   max_length = max(max_length, len(input_ids))\n",
        "\n",
        "# # add 35 incase any instance in test or valid set are larger, could catch some issues, also cuz it rounds to 300\n",
        "# max_length = max_length + 35\n",
        "\n",
        "# print(max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "JLYxCVJ9FlqQ"
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = 300\n",
        "\n",
        "BATCH_SIZE = 256 # 32\n",
        "\n",
        "EVAL_BATCH_SIZE = BATCH_SIZE * 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "gUk0t4pyyEBF"
      },
      "outputs": [],
      "source": [
        "def batch_encode(tokenizer, texts, batch_size=BATCH_SIZE, max_length=MAX_LENGTH):\n",
        "\n",
        "  input_ids_batch = []\n",
        "  attention_mask = []\n",
        "\n",
        "  for i in range(0, len(texts), batch_size): \n",
        "    batch = texts[i:i+batch_size]\n",
        "    \n",
        "    inputs = tokenizer.batch_encode_plus(\n",
        "      batch,\n",
        "      max_length=MAX_LENGTH, # set the length of the sequences\n",
        "      add_special_tokens=True, # add [CLS] and [SEP] tokens\n",
        "      return_attention_mask=True,\n",
        "      truncation=True, # yells if not set but padding is done\n",
        "      return_token_type_ids=False, # not needed for this type of ML task\n",
        "      pad_to_max_length=True, # add 0 pad tokens to the sequences less than max_length\n",
        "      return_tensors='np'\n",
        "      )\n",
        "    input_ids_batch.extend(inputs['input_ids'])\n",
        "    attention_mask.extend(inputs['attention_mask'])\n",
        "  \n",
        "  return tf.convert_to_tensor(input_ids_batch), tf.convert_to_tensor(attention_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iJryn7Bg1mv",
        "outputId": "25e70913-28f3-4911-fa06-820213c06e6b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/mani/miniforge3/envs/cloud/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2212: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Encode X_train\n",
        "X_train_ids, X_train_attention = batch_encode(tokenizer, X_train.tolist())\n",
        "\n",
        "# Encode X_valid\n",
        "X_valid_ids, X_valid_attention = batch_encode(tokenizer, X_valid.tolist())\n",
        "\n",
        "# Encode X_test\n",
        "X_test_ids, X_test_attention = batch_encode(tokenizer, X_test.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_ids = np.asarray(X_train_ids)\n",
        "X_train_attention = np.asarray(X_train_attention)\n",
        "X_valid_ids = np.asarray(X_valid_ids)\n",
        "X_valid_attention = np.asarray(X_valid_attention)\n",
        "X_test_ids =  np.asarray(X_test_ids)\n",
        "X_test_attention = np.asarray(X_test_attention)\n",
        "# input_ids = np.asarray(input_ids)\n",
        "# attention_mask = np.asarray(attention_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "QriKui1hyEF5"
      },
      "outputs": [],
      "source": [
        "def create_model(transformer, max_length=MAX_LENGTH):    \n",
        "    # Define input layers\n",
        "    \n",
        "    # This is the input for the tokens themselves(words from the dataset after encoding):\n",
        "    input_ids_layer = tf.keras.layers.Input(shape=(max_length,), \n",
        "                                            name='input_ids', \n",
        "                                            dtype='int32')\n",
        "\n",
        "    # attention_mask - is a binary mask which tells BERT which tokens to attend and which not to attend.\n",
        "    # Encoder will add the 0 tokens to the some sequence which smaller than MAX_SEQUENCE_LENGTH, \n",
        "    # and attention_mask, in this case, tells BERT where is the token from the original data and where is 0 pad token:\n",
        "\n",
        "    input_attention_layer = tf.keras.layers.Input(shape=(max_length,), \n",
        "                                                  name='input_attention', \n",
        "                                                  dtype='int32')\n",
        "    # Use previous inputs as BERT inputs:\n",
        "    bert = transformer([input_ids_layer, input_attention_layer])[0]\n",
        "\n",
        "    # We can also add dropout as regularization technique:\n",
        "    #output = tf.keras.layers.Dropout(rate=0.15)(output)\n",
        "\n",
        "    # Provide number of classes to the final layer:\n",
        "    output = tf.keras.layers.Dense(1, activation='softmax')(bert)\n",
        "\n",
        "    # Final model:\n",
        "    model = tf.keras.models.Model([input_ids_layer, input_attention_layer], outputs=output)\n",
        "\n",
        "\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6CigGzYyEDB",
        "outputId": "16c9da7d-2015-4651-f6e3-ccfbbedf1572"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# bert_model = trfs.TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "bert_model = trfs.AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
        "from transformers import TFDistilBertModel, DistilBertConfig\n",
        "\n",
        "config = DistilBertConfig(dropout=0.2, attention_dropout=0.2, output_hidden_states=False)\n",
        "# bert_model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'size'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/5p/hmf55qt95d13rj6fqf3q64th0000gn/T/ipykernel_79394/2265747317.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/var/folders/5p/hmf55qt95d13rj6fqf3q64th0000gn/T/ipykernel_79394/4020722147.py\u001b[0m in \u001b[0;36mcreate_model\u001b[0;34m(transformer, max_length)\u001b[0m\n\u001b[1;32m     15\u001b[0m                                                   dtype='int32')\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Use previous inputs as BERT inputs:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mbert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_ids_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_attention_layer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# We can also add dropout as regularization technique:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniforge3/envs/cloud/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniforge3/envs/cloud/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1528\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1530\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m   1531\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1532\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniforge3/envs/cloud/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniforge3/envs/cloud/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    942\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You cannot specify both input_ids and inputs_embeds at the same time\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 944\u001b[0;31m             \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    945\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
          ]
        }
      ],
      "source": [
        "model = create_model(bert_model, MAX_LENGTH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_ids = np.asarray(X_train_ids)\n",
        "X_train_attention = np.asarray(X_train_attention)\n",
        "# X_valid_ids = np.asarray(X_valid_ids)\n",
        "# X_valid_attention = np.asarray(X_valid_attention)\n",
        "print(type(X_train_ids))\n",
        "print(type(X_valid_attention))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDmhtFFnL25H",
        "outputId": "49ee81d2-20ac-4aa2-96f3-b6ee29f9d168"
      },
      "outputs": [],
      "source": [
        "model.fit(\n",
        "    x=[X_train_ids, X_train_attention],\n",
        "    y=y_train,\n",
        "    epochs=3,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_data=([X_valid_ids, X_valid_attention], y_valid)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Create and train the model\n",
        "\n",
        "#Create LSTM concurrent NN\n",
        "#train the model\n",
        "       \n",
        "#fix random seed for reproducibility\n",
        "np.random.seed(7)\n",
        "\n",
        "#load the dataset but only keep the top n words, zero the rest\n",
        "top_words = 5000\n",
        "# (X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
        "\n",
        "#truncate and pad input sequences\n",
        "# max_review_length = 500\n",
        "# X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
        "# X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
        "\n",
        "#create model\n",
        "# model = Sequential()\n",
        "# model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
        "# model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "# model.add(MaxPooling1D(pool_size=2))\n",
        "# model.add(LSTM(100))\n",
        "# model.add(Dense(1, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHLvdZexwrTA"
      },
      "outputs": [],
      "source": [
        "# instead of doing it that way let's also see the lstm\n",
        "def use_lstm(top_words=top_words):\n",
        "  embedding_vector_length = 32\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(top_words, embedding_vector_length, input_length=MAX_LENGTH))\n",
        "  model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "  model.add(MaxPooling1D(pool_size=2))\n",
        "  model.add(LSTM(100))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) #need to write code for AUROC, F1, and precision\n",
        "  print(model.summary())\n",
        "  model.fit(X_train, y_train, epochs=3, batch_size=64) #may consider changing # of epochs for speed\n",
        "  # model.train() Error: AttributeError: 'Sequential' object has no attribute 'train'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = use_lstm()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "yhat_probs = model.predict(X_test, verbose=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "yhat_classes = (model.predict(X_test) > 0.5).astype(\"int32\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(yhat_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "yhat_classes = model.predict_classes(X_test, verbose=0) #AttributeError: 'Sequential' object has no attribute 'predict_classes'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "yhat_probs = yhat_probs[:, 0]\n",
        "yhat_classes = yhat_classes[:, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Performance Metrics: Accuracy, precision, recall, F1, cohen's kappa, AUROC, and confusion matrix\n",
        "\n",
        "# predict probabilities for test set\n",
        "yhat_probs = model.predict(X_test, verbose=0)\n",
        "# predict classes for test set\n",
        "# yhat_classes = model.predict_classes(X_test, verbose=0) #AttributeError: 'Sequential' object has no attribute 'predict_classes'\n",
        "yhat_classes = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "\n",
        "# reduce to 1d array\n",
        "yhat_probs = yhat_probs[:, 0]\n",
        "yhat_classes = yhat_classes[:, 0]\n",
        "\n",
        "# tp = true positive, tn = true negative, fp = false positive, fn = false negative, p = positive, n = negative\n",
        "# accuracy: (tp + tn) / (p + n)\n",
        "accuracy = accuracy_score(y_test, yhat_classes)\n",
        "print('Accuracy: %f' % accuracy)\n",
        "\n",
        "# precision tp / (tp + fp)\n",
        "precision = precision_score(y_test, yhat_classes)\n",
        "print('Precision: %f' % precision)\n",
        "\n",
        "# recall: tp / (tp + fn)\n",
        "recall = recall_score(y_test, yhat_classes)\n",
        "print('Recall: %f' % recall)\n",
        "\n",
        "# f1: 2 tp / (2 tp + fp + fn)\n",
        "f1 = f1_score(y_test, yhat_classes)\n",
        "print('F1 score: %f' % f1)\n",
        "\n",
        "# cohen's kappa\n",
        "kappa = cohen_kappa_score(y_test, yhat_classes)\n",
        "\n",
        "print('Cohens kappa: %f' % kappa)\n",
        "# AUROC\n",
        "auc = roc_auc_score(y_test, yhat_probs)\n",
        "print('AUROC: %f' % auc)\n",
        "\n",
        "# confusion matrix\n",
        "matrix = confusion_matrix(y_test, yhat_classes)\n",
        "print('Confusion matrix: ', matrix)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "tf.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
