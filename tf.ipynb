{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duyiYKj_clff"
      },
      "source": [
        "Importing Libraries and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOXWY9iavpyb",
        "outputId": "68d04c0a-d83e-4b08-f913-0557ddcee6bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Requirement already satisfied: utils in /Users/mani/miniforge3/envs/cloud/lib/python3.9/site-packages (1.0.1)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Requirement already satisfied: setuptools in /Users/mani/miniforge3/envs/cloud/lib/python3.9/site-packages (59.5.0)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Collecting numpy==1.19\n",
            "  Downloading numpy-1.19.0.zip (7.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.3 MB 6.3 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
            "\u001b[?25hBuilding wheels for collected packages: numpy\n",
            "  Building wheel for numpy (PEP 517) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for numpy: filename=numpy-1.19.0-cp39-cp39-macosx_11_0_arm64.whl size=3715524 sha256=28ff86294d654511e040d4c8b7614f338179454f1cb820962a3b3b80018876c1\n",
            "  Stored in directory: /Users/mani/Library/Caches/pip/wheels/d2/d9/5e/6ce8602ab8be91874fa00a1286a82f4430d7b0f9daece5fa3e\n",
            "Successfully built numpy\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.4\n",
            "    Uninstalling numpy-1.21.4:\n",
            "      Successfully uninstalled numpy-1.21.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "transformers 4.12.5 requires huggingface-hub<1.0,>=0.1.0, but you have huggingface-hub 0.0.17 which is incompatible.\n",
            "tensorflow 2.6.0 requires flatbuffers~=1.12.0, but you have flatbuffers 20210226132247 which is incompatible.\n",
            "tensorflow 2.6.0 requires numpy~=1.19.2, but you have numpy 1.19.0 which is incompatible.\n",
            "pandas 1.3.4 requires numpy>=1.20.0, but you have numpy 1.19.0 which is incompatible.\u001b[0m\n",
            "Successfully installed numpy-1.19.5\n"
          ]
        }
      ],
      "source": [
        "# Mohammad Malik & Adair Maynard\n",
        "# Task 5: MAMI\n",
        "# CMSC 516: NLP at VCU, Dr. Bridget McInnes\n",
        "\n",
        "!pip install utils\n",
        "!pip install setuptools\n",
        "!pip install numpy==1.19\n",
        "\n",
        "import os\n",
        "import io\n",
        "import utils\n",
        "import re\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from math import floor\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "import tensorflow as tf\n",
        "import transformers as trfs\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "# from google.colab import auth\n",
        "# from googleapiclient.discovery import build\n",
        "# from io import FileIO\n",
        "# from googleapiclient.http import MediaIoBaseDownload\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation \n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "aPec8z7DwFCx",
        "outputId": "ea52c0b6-7d84-4047-8173-a53844769eb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.12.5\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'2.6.0'"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(trfs.__version__)\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mE0G90Czwq4z",
        "outputId": "e71d1b4d-8a18-431a-d5f4-a1d0875a6ce7"
      },
      "outputs": [],
      "source": [
        "# TODO uncomment cell if using Google Colab\n",
        "#  Get the files from the google drive\n",
        "# auth.authenticate_user()\n",
        "# drive_service = build('drive', 'v3')\n",
        "\n",
        "# # Get data file\n",
        "# file_id = '1sgE4tBtZwE-M9Y6rfLu_FlwI6VpnaUo-'  # Training file on the Google Drive\n",
        "# downloaded = io.FileIO(\"training.csv\", 'w')\n",
        "# request = drive_service.files().get_media(fileId=file_id)\n",
        "# downloader = MediaIoBaseDownload(downloaded, request)\n",
        "# done = False\n",
        "# while done is False:\n",
        "#   status, done = downloader.next_chunk()\n",
        "#   print(\"Download {}%.\".format(int(status.progress() * 100)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwG-Ix8Mwr2k",
        "outputId": "d1cdde71-4bac-4e6b-b66c-30ad5dabb221"
      },
      "outputs": [],
      "source": [
        "#  Load Tokenizer\n",
        "# TODO look at the finetuned model because it resulted in a lot of UNKNOWN words for capitals and doesnt know a lot of words\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\") #Hate-speech-CNERG/dehatebert-mono-english"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "GmBalKfrwr0E",
        "outputId": "eb7d23ce-c80c-478f-a052-1f97bc4cdf6d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>misogynous</th>\n",
              "      <th>shaming</th>\n",
              "      <th>stereotype</th>\n",
              "      <th>objectification</th>\n",
              "      <th>violence</th>\n",
              "      <th>Text Transcription</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Milk Milk.zip</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>ROSES ARE RED, VIOLETS ARE BLUE IF YOU DON'T S...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1000.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>BREAKING NEWS: Russia releases photo of DONALD...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10000.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>MAN SEEKING WOMAN Ignad 18 O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10006.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Me explaining the deep lore of. J.R.R. Tolkein...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   file_name  misogynous  shaming  stereotype  objectification  violence  \\\n",
              "0      1.jpg           0        0           0                0         0   \n",
              "1     10.jpg           1        0           0                0         1   \n",
              "2   1000.jpg           0        0           0                0         0   \n",
              "3  10000.jpg           0        0           0                0         0   \n",
              "4  10006.jpg           0        0           0                0         0   \n",
              "\n",
              "                                  Text Transcription  \n",
              "0                                      Milk Milk.zip  \n",
              "1  ROSES ARE RED, VIOLETS ARE BLUE IF YOU DON'T S...  \n",
              "2  BREAKING NEWS: Russia releases photo of DONALD...  \n",
              "3                       MAN SEEKING WOMAN Ignad 18 O  \n",
              "4  Me explaining the deep lore of. J.R.R. Tolkein...  "
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load dataset\n",
        "df = pd.read_csv(\"TRAINING/training.csv\", delimiter=\"\\t\")\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "IMbXswpnwrxu",
        "outputId": "2e7eed66-14b2-4da1-aa2c-9c06d5ba7f69"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>misogynous</th>\n",
              "      <th>Text Transcription</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>Milk Milk.zip</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>ROSES ARE RED, VIOLETS ARE BLUE IF YOU DON'T S...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1000.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>BREAKING NEWS: Russia releases photo of DONALD...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10000.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>MAN SEEKING WOMAN Ignad 18 O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10006.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>Me explaining the deep lore of. J.R.R. Tolkein...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>15002.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>WAITING FOR THE END OF THE COVID  imgflip.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>15003.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>SMART WOMEN ARE AROUND  imgflip.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>15004.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>GOOD GIRLS ARE BEHIND THE CORNER  imgflip.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>15005.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>COOKING FOR MY WIFE  imgflip.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>15006.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>LISTEN TOMORROW WILL BE MONDAY imgflip.com FRO...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      file_name  misogynous                                 Text Transcription\n",
              "0         1.jpg           0                                      Milk Milk.zip\n",
              "1        10.jpg           1  ROSES ARE RED, VIOLETS ARE BLUE IF YOU DON'T S...\n",
              "2      1000.jpg           0  BREAKING NEWS: Russia releases photo of DONALD...\n",
              "3     10000.jpg           0                       MAN SEEKING WOMAN Ignad 18 O\n",
              "4     10006.jpg           0  Me explaining the deep lore of. J.R.R. Tolkein...\n",
              "...         ...         ...                                                ...\n",
              "9995  15002.jpg           0      WAITING FOR THE END OF THE COVID  imgflip.com\n",
              "9996  15003.jpg           0                SMART WOMEN ARE AROUND  imgflip.com\n",
              "9997  15004.jpg           0      GOOD GIRLS ARE BEHIND THE CORNER  imgflip.com\n",
              "9998  15005.jpg           0                   COOKING FOR MY WIFE  imgflip.com\n",
              "9999  15006.jpg           0  LISTEN TOMORROW WILL BE MONDAY imgflip.com FRO...\n",
              "\n",
              "[10000 rows x 3 columns]"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# get rid of multi classes and only keep misogyny\n",
        "X = df.drop(columns=['shaming', 'objectification', 'stereotype', 'violence'])\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3tyXdLBwrur",
        "outputId": "9d9b573c-e02a-47a6-8eeb-0a7eb3aa9f84"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /Users/mani/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /Users/mani/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/mani/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# preprocess text then pass into model\n",
        "\n",
        "# download NLTK stopwords, punctuation and wordLemmatizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# clean/preprocess dataset instances\n",
        "def clean_data(trans):\n",
        "  words = stopwords.words('english')\n",
        "  filtered_words = \" \".join([w for w in trans.split() if not w.lower() in words]) # might need to be trans.lower().split()\n",
        "  url = ''.join(re.sub(r'http\\S+', '', filtered_words))\n",
        "  almost = ''.join(re.sub(r'www\\S+', '', url))\n",
        "  cleaned = ''.join(re.sub(r'(\\w+\\.\\w+)', '', almost))\n",
        "  punct = ''.join(ch for ch in cleaned if ch not in punctuation)\n",
        "  wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "  word_tokens = nltk.word_tokenize(punct)\n",
        "  lemmatized_word = [wordnet_lemmatizer.lemmatize(word) for word in word_tokens]\n",
        "\n",
        "  word_joined = \" \".join(lemmatized_word)\n",
        "  \n",
        "  return word_joined"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "LVWi-IPfwrsK"
      },
      "outputs": [],
      "source": [
        "# apply preprocessing to dataset\n",
        "X['Cleaned Text'] = X['Text Transcription'].apply(clean_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "jPGSIYRkwrnH",
        "outputId": "bd0bf882-c74d-4128-97bc-2c53d9ffdeee"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Misogynous</th>\n",
              "      <th>Cleaned_Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Milk</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>ROSES RED VIOLETS BLUE SAY YES ILL RAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>BREAKING NEWS Russia release photo DONALD TRUM...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>MAN SEEKING WOMAN Ignad 18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>explaining deep lore of R Tolkeins world Arda ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>0</td>\n",
              "      <td>WAITING END COVID</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>0</td>\n",
              "      <td>SMART WOMEN AROUND</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>0</td>\n",
              "      <td>GOOD GIRLS BEHIND CORNER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>0</td>\n",
              "      <td>COOKING WIFE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>0</td>\n",
              "      <td>LISTEN TOMORROW MONDAY FROM USERMENSWEARHOUSE</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Misogynous                                       Cleaned_Text\n",
              "0              0                                               Milk\n",
              "1              1            ROSES RED VIOLETS BLUE SAY YES ILL RAPE\n",
              "2              0  BREAKING NEWS Russia release photo DONALD TRUM...\n",
              "3              0                         MAN SEEKING WOMAN Ignad 18\n",
              "4              0  explaining deep lore of R Tolkeins world Arda ...\n",
              "...          ...                                                ...\n",
              "9995           0                                  WAITING END COVID\n",
              "9996           0                                 SMART WOMEN AROUND\n",
              "9997           0                           GOOD GIRLS BEHIND CORNER\n",
              "9998           0                                       COOKING WIFE\n",
              "9999           0      LISTEN TOMORROW MONDAY FROM USERMENSWEARHOUSE\n",
              "\n",
              "[10000 rows x 2 columns]"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# create new variable that solely consists of cleaned text and labels\n",
        "X_bert = pd.DataFrame({\n",
        "    # 'File_Name':X['file_name'],\n",
        "    'Misogynous':X['misogynous'],\n",
        "    # 'Alpha':['a']*X.shape[0],\n",
        "    'Cleaned_Text':X['Cleaned Text']\n",
        "})\n",
        "X_bert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Tkmquqjy1xf",
        "outputId": "fa5fa233-8610-4477-b04b-06da1e14b615"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(10000,)\n",
            "[0 1 0 ... 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "# get list of sentences and labels\n",
        "sentences = X_bert.Cleaned_Text.values\n",
        "print(sentences.shape)\n",
        "\n",
        "labels = X_bert.Misogynous.values\n",
        "print(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "fFPFrM2gwrkp"
      },
      "outputs": [],
      "source": [
        "# split dataset into train and test to be passed into models, 85% and 15%\n",
        "X_train, X_rem, y_train, y_rem = train_test_split(sentences, labels, train_size=0.85)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "iK3OlSxKwriR"
      },
      "outputs": [],
      "source": [
        "# split dataset further into valid and test\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_rem, y_rem, train_size=0.333)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2Sc2B17wrdL",
        "outputId": "c6b26cf1-1641-4952-c486-22fec1b19e81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "orig:  INTERNATIONAL WOMENS DAY CONGRATULATIONS BORN TWO X CHROMOSOMES MUST COST LOT WORK\n",
            "Tokenized:  ['international', 'women', '##s', 'day', 'congratulations', 'born', 'two', 'x', 'chromosomes', 'must', 'cost', 'lot', 'work']\n",
            "Token IDs:  [2248, 2308, 2015, 2154, 23156, 2141, 2048, 1060, 26874, 2442, 3465, 2843, 2147]\n"
          ]
        }
      ],
      "source": [
        "# testing bert tokenizer\n",
        "print('orig: ', X_train[1])\n",
        "print('Tokenized: ', tokenizer.tokenize(X_train[1]))\n",
        "print(\"Token IDs: \", tokenizer.convert_tokens_to_ids(tokenizer.tokenize(X_train[1])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "JLYxCVJ9FlqQ"
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = 300\n",
        "\n",
        "BATCH_SIZE = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "gUk0t4pyyEBF"
      },
      "outputs": [],
      "source": [
        "def batch_encode(tokenizer, texts, batch_size=BATCH_SIZE, max_length=MAX_LENGTH):\n",
        "\n",
        "  input_ids_batch = []\n",
        "  attention_mask = []\n",
        "\n",
        "  for i in range(0, len(texts), batch_size): \n",
        "    batch = texts[i:i+batch_size]\n",
        "    \n",
        "    inputs = tokenizer.batch_encode_plus(\n",
        "      batch,\n",
        "      max_length=MAX_LENGTH, # set the length of the sequences\n",
        "      add_special_tokens=True, # add [CLS] and [SEP] tokens\n",
        "      return_attention_mask=True,\n",
        "      truncation=True, # yells if not set but padding is done\n",
        "      return_token_type_ids=False, # not needed for this type of ML task\n",
        "      pad_to_max_length=True, # add 0 pad tokens to the sequences less than max_length\n",
        "      return_tensors='tf'\n",
        "      )\n",
        "    input_ids_batch.extend(inputs['input_ids'])\n",
        "    attention_mask.extend(inputs['attention_mask'])\n",
        "  \n",
        "  return tf.convert_to_tensor(input_ids_batch), tf.convert_to_tensor(attention_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iJryn7Bg1mv",
        "outputId": "25e70913-28f3-4911-fa06-820213c06e6b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/mani/miniforge3/envs/cloud/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2212: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Encode X_train\n",
        "X_train_ids, X_train_attention = batch_encode(tokenizer, X_train.tolist())\n",
        "\n",
        "# Encode X_valid\n",
        "X_valid_ids, X_valid_attention = batch_encode(tokenizer, X_valid.tolist())\n",
        "\n",
        "# Encode X_test\n",
        "X_test_ids, X_test_attention = batch_encode(tokenizer, X_test.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_ids = np.asarray(X_train_ids)\n",
        "X_train_attention = np.asarray(X_train_attention)\n",
        "X_valid_ids = np.asarray(X_valid_ids)\n",
        "X_valid_attention = np.asarray(X_valid_attention)\n",
        "X_test_ids =  np.asarray(X_test_ids)\n",
        "X_test_attention = np.asarray(X_test_attention)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "QriKui1hyEF5"
      },
      "outputs": [],
      "source": [
        "def create_model(transformer, max_length=MAX_LENGTH):    \n",
        "    # Define input layers\n",
        "    \n",
        "    # This is the input for the tokens themselves(words from the dataset after encoding):\n",
        "    input_ids_layer = tf.keras.layers.Input(shape=(max_length,), \n",
        "                                            name='input_ids', \n",
        "                                            dtype='int32')\n",
        "\n",
        "    # attention_mask - is a binary mask which tells BERT which tokens to attend and which not to attend.\n",
        "    # Encoder will add the 0 tokens to the some sequence which smaller than MAX_SEQUENCE_LENGTH, \n",
        "    # and attention_mask, in this case, tells BERT where is the token from the original data and where is 0 pad token:\n",
        "\n",
        "    input_attention_layer = tf.keras.layers.Input(shape=(max_length,), \n",
        "                                                  name='input_attention', \n",
        "                                                  dtype='int32')\n",
        "    # Use previous inputs as BERT inputs:\n",
        "    bert = transformer([input_ids_layer, input_attention_layer])[0]\n",
        "\n",
        "    # We can also add dropout as regularization technique:\n",
        "    dropped = tf.keras.layers.Dropout(rate=0.15)(bert)\n",
        "\n",
        "    # Provide number of classes to the final layer:\n",
        "    output = tf.keras.layers.Dense(1, activation='sigmoid')(dropped)\n",
        "\n",
        "    # Final model:\n",
        "    model = tf.keras.models.Model([input_ids_layer, input_attention_layer], outputs=output)\n",
        "\n",
        "\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6CigGzYyEDB",
        "outputId": "16c9da7d-2015-4651-f6e3-ccfbbedf1572"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_projector', 'vocab_layer_norm', 'activation_13', 'vocab_transform']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "# Load Model\n",
        "\n",
        "# bert_model = trfs.TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "# model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\") #Hate-speech-CNERG/dehatebert-mono-english\n",
        "\n",
        "from transformers import TFDistilBertModel, DistilBertConfig\n",
        "\n",
        "config = DistilBertConfig(dropout=0.2, attention_dropout=0.2, output_hidden_states=False)\n",
        "bert_model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = create_model(bert_model, MAX_LENGTH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_ids (InputLayer)          [(None, 300)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_attention (InputLayer)    [(None, 300)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_distil_bert_model_1 (TFDisti TFBaseModelOutput(la 66362880    input_ids[0][0]                  \n",
            "                                                                 input_attention[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_39 (Dropout)            (None, 300, 768)     0           tf_distil_bert_model_1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 300, 1)       769         dropout_39[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 66,363,649\n",
            "Trainable params: 66,363,649\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDmhtFFnL25H",
        "outputId": "49ee81d2-20ac-4aa2-96f3-b6ee29f9d168"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "  5/133 [>.............................] - ETA: 59:50 - loss: 0.6911 - accuracy: 0.5515"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "    x=[X_train_ids, X_train_attention],\n",
        "    y=y_train,\n",
        "    epochs=2,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_data=([X_valid_ids, X_valid_attention], y_valid)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "yhat_probs = model.predict([X_test_ids, X_test_attention], verbose=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[[1]\n",
            "  [1]\n",
            "  [1]\n",
            "  ...\n",
            "  [1]\n",
            "  [1]\n",
            "  [1]]\n",
            "\n",
            " [[1]\n",
            "  [1]\n",
            "  [1]\n",
            "  ...\n",
            "  [1]\n",
            "  [1]\n",
            "  [1]]\n",
            "\n",
            " [[0]\n",
            "  [0]\n",
            "  [0]\n",
            "  ...\n",
            "  [0]\n",
            "  [0]\n",
            "  [0]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0]\n",
            "  [0]\n",
            "  [0]\n",
            "  ...\n",
            "  [0]\n",
            "  [0]\n",
            "  [0]]\n",
            "\n",
            " [[0]\n",
            "  [0]\n",
            "  [0]\n",
            "  ...\n",
            "  [0]\n",
            "  [0]\n",
            "  [0]]\n",
            "\n",
            " [[1]\n",
            "  [1]\n",
            "  [1]\n",
            "  ...\n",
            "  [1]\n",
            "  [1]\n",
            "  [1]]]\n"
          ]
        }
      ],
      "source": [
        "yhat_classes = (model.predict([X_test_ids, X_test_attention]) > 0.5).astype(\"int32\")\n",
        "print(yhat_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "yhat_probs = yhat_probs[:, 0]\n",
        "yhat_classes = yhat_classes[:, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.784216\n",
            "Precision: 0.788136\n",
            "Recall: 0.762295\n",
            "F1 score: 0.775000\n",
            "Cohens kappa: 0.567817\n",
            "AUROC: 0.861499\n",
            "Confusion matrix:  [[413 100]\n",
            " [116 372]]\n"
          ]
        }
      ],
      "source": [
        "#Performance Metrics: Accuracy, precision, recall, F1, cohen's kappa, AUROC, and confusion matrix\n",
        "\n",
        "# predict probabilities for test set\n",
        "yhat_probs = model.predict([X_test_ids, X_test_attention], verbose=0)\n",
        "# predict classes for test set\n",
        "# yhat_classes = model.predict_classes(X_test, verbose=0) #AttributeError: 'Sequential' object has no attribute 'predict_classes'\n",
        "yhat_classes = (model.predict([X_test_ids, X_test_attention]) > 0.5).astype(\"int32\")\n",
        "\n",
        "# reduce to 1d array\n",
        "yhat_probs = yhat_probs[:, 0]\n",
        "yhat_classes = yhat_classes[:, 0]\n",
        "\n",
        "# tp = true positive, tn = true negative, fp = false positive, fn = false negative, p = positive, n = negative\n",
        "# accuracy: (tp + tn) / (p + n)\n",
        "accuracy = accuracy_score(y_test, yhat_classes)\n",
        "print('Accuracy: %f' % accuracy)\n",
        "\n",
        "# precision tp / (tp + fp)\n",
        "precision = precision_score(y_test, yhat_classes)\n",
        "print('Precision: %f' % precision)\n",
        "\n",
        "# recall: tp / (tp + fn)\n",
        "recall = recall_score(y_test, yhat_classes)\n",
        "print('Recall: %f' % recall)\n",
        "\n",
        "# f1: 2 tp / (2 tp + fp + fn)\n",
        "f1 = f1_score(y_test, yhat_classes)\n",
        "print('F1 score: %f' % f1)\n",
        "\n",
        "# cohen's kappa\n",
        "kappa = cohen_kappa_score(y_test, yhat_classes)\n",
        "\n",
        "print('Cohens kappa: %f' % kappa)\n",
        "# AUROC\n",
        "auc = roc_auc_score(y_test, yhat_probs)\n",
        "print('AUROC: %f' % auc)\n",
        "\n",
        "# confusion matrix\n",
        "matrix = confusion_matrix(y_test, yhat_classes)\n",
        "print('Confusion matrix: ', matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history_dict = history.history\n",
        "print(history_dict.keys())\n",
        "\n",
        "acc = history_dict['binary_accuracy']\n",
        "val_acc = history_dict['val_binary_accuracy']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.subplot(2, 1, 1)\n",
        "# r is for \"solid red line\"\n",
        "plt.plot(epochs, loss, 'r', label='Training loss')\n",
        "# b is for \"solid blue line\"\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "# plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(epochs, acc, 'r', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "OHLvdZexwrTA"
      },
      "outputs": [],
      "source": [
        "# instead of doing it that way let's also see the lstm\n",
        "def use_lstm():\n",
        "  embedding_vector_length = 32\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(5000, embedding_vector_length, input_length=MAX_LENGTH))\n",
        "  model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "  model.add(MaxPooling1D(pool_size=2))\n",
        "  model.add(LSTM(100))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = use_lstm()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 300, 32)           9600      \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 300, 32)           3104      \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 150, 32)           0         \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 66,005\n",
            "Trainable params: 66,005\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "ename": "InvalidArgumentError",
          "evalue": " indices[58,1] = 2047 is not in [0, 300)\n\t [[node sequential/embedding/embedding_lookup (defined at var/folders/5p/hmf55qt95d13rj6fqf3q64th0000gn/T/ipykernel_87743/2610551650.py:1) ]] [Op:__inference_train_function_102775]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node sequential/embedding/embedding_lookup:\n sequential/embedding/embedding_lookup/101505 (defined at Users/mani/miniforge3/envs/cloud/lib/python3.9/contextlib.py:119)\n\nFunction call stack:\ntrain_function\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/5p/hmf55qt95d13rj6fqf3q64th0000gn/T/ipykernel_87743/2610551650.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_train_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_attention\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#may consider changing # of epochs for speed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m~/miniforge3/envs/cloud/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniforge3/envs/cloud/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniforge3/envs/cloud/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    948\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniforge3/envs/cloud/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3037\u001b[0m       (graph_function,\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3039\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniforge3/envs/cloud/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1963\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m~/miniforge3/envs/cloud/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniforge3/envs/cloud/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m:  indices[58,1] = 2047 is not in [0, 300)\n\t [[node sequential/embedding/embedding_lookup (defined at var/folders/5p/hmf55qt95d13rj6fqf3q64th0000gn/T/ipykernel_87743/2610551650.py:1) ]] [Op:__inference_train_function_102775]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node sequential/embedding/embedding_lookup:\n sequential/embedding/embedding_lookup/101505 (defined at Users/mani/miniforge3/envs/cloud/lib/python3.9/contextlib.py:119)\n\nFunction call stack:\ntrain_function\n"
          ]
        }
      ],
      "source": [
        "history2 = model.fit([X_train_ids, X_train_attention], y_train, epochs=3, batch_size=64) #may consider changing # of epochs for speed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "yhat_probs = model.predict([X_test_ids, X_test_attention], verbose=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "yhat_classes = (model.predict([X_test_ids, X_test_attention]) > 0.5).astype(\"int32\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(yhat_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "yhat_classes = model.predict_classes([X_test_ids, X_test_attention], verbose=0) #AttributeError: 'Sequential' object has no attribute 'predict_classes'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "yhat_probs = yhat_probs[:, 0]\n",
        "yhat_classes = yhat_classes[:, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Performance Metrics: Accuracy, precision, recall, F1, cohen's kappa, AUROC, and confusion matrix\n",
        "\n",
        "# predict probabilities for test set\n",
        "yhat_probs = model.predict([X_test_ids, X_test_attention], verbose=0)\n",
        "# predict classes for test set\n",
        "# yhat_classes = model.predict_classes(X_test, verbose=0) #AttributeError: 'Sequential' object has no attribute 'predict_classes'\n",
        "yhat_classes = (model.predict([X_test_ids, X_test_attention]) > 0.5).astype(\"int32\")\n",
        "\n",
        "# reduce to 1d array\n",
        "yhat_probs = yhat_probs[:, 0]\n",
        "yhat_classes = yhat_classes[:, 0]\n",
        "\n",
        "# tp = true positive, tn = true negative, fp = false positive, fn = false negative, p = positive, n = negative\n",
        "# accuracy: (tp + tn) / (p + n)\n",
        "accuracy = accuracy_score(y_test, yhat_classes)\n",
        "print('Accuracy: %f' % accuracy)\n",
        "\n",
        "# precision tp / (tp + fp)\n",
        "precision = precision_score(y_test, yhat_classes)\n",
        "print('Precision: %f' % precision)\n",
        "\n",
        "# recall: tp / (tp + fn)\n",
        "recall = recall_score(y_test, yhat_classes)\n",
        "print('Recall: %f' % recall)\n",
        "\n",
        "# f1: 2 tp / (2 tp + fp + fn)\n",
        "f1 = f1_score(y_test, yhat_classes)\n",
        "print('F1 score: %f' % f1)\n",
        "\n",
        "# cohen's kappa\n",
        "kappa = cohen_kappa_score(y_test, yhat_classes)\n",
        "\n",
        "print('Cohens kappa: %f' % kappa)\n",
        "# AUROC\n",
        "auc = roc_auc_score(y_test, yhat_probs)\n",
        "print('AUROC: %f' % auc)\n",
        "\n",
        "# confusion matrix\n",
        "matrix = confusion_matrix(y_test, yhat_classes)\n",
        "print('Confusion matrix: ', matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history_dict = history2.history\n",
        "print(history_dict.keys())\n",
        "\n",
        "acc = history_dict['binary_accuracy']\n",
        "val_acc = history_dict['val_binary_accuracy']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.subplot(2, 1, 1)\n",
        "# r is for \"solid red line\"\n",
        "plt.plot(epochs, loss, 'r', label='Training loss')\n",
        "# b is for \"solid blue line\"\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "# plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(epochs, acc, 'r', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "tf.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
